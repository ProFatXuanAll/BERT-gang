r"""Fine-tune distillation objective.

Usage:
    loss = soft_target_loss(...)
    loss = distill(...)
"""

# built-in modules

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

# 3rd party modules

import torch
import torch.nn
import torch.nn.functional as F


def soft_target_loss(
        pred: torch.FloatTensor,
        soft_target: torch.FloatTensor
) -> torch.FloatTensor:
    r"""Soft target loss.

    Formula:
        -\sum_{i = 1}^c p_i * \log q_i,
        Where `c` is number of class, `p_i` is soft_target generated by
        teacher model, and `q_i` is student model prediction distribution.

    Args:
        pred:
            Student model prediction distribution.
            Shape: (batch, num_class).
        soft_target:
            Teacher model prediction distribution.
            Shape: (batch, num_class).

    Returns:
        Soft target loss as state in formula section.
        See Hinton, G. (2014). Distilling the Knowledge in a Neural Network.
    """

    loss = soft_target * pred.log()
    loss = -loss.sum(dim=-1)

    return loss.mean()


def distill(
        pred: torch.FloatTensor,
        hard_target: torch.LongTensor,
        soft_target: torch.FloatTensor
) -> torch.FloatTensor:
    r"""Hard target + Soft target loss.

    Formula:
        (-y_j * \log q_j) + (-\sum_{i = 1}^c p_i * \log q_i),
        Where `y_j` is the actual label, `j` is the actual label index,
        `c` is number of class, `p_i` is soft_target generated by
        teacher model, and `q_i` is student model prediction distribution.

    Args:
        pred:
            Student model prediction distribution.
            Shape: (batch, num_class).
        hard_target:
            Actual label for cross-entropy loss.
            Shape: (batch,).
        soft_target:
            Teacher model prediction distribution.
            Shape: (batch, num_class).

    Returns:
        Hard target + soft target loss as state in formula section.
        See Hinton, G. (2014). Distilling the Knowledge in a Neural Network.
    """
    return (
        F.cross_entropy(pred, hard_target) +
        soft_target_loss(pred, soft_target)
    )
