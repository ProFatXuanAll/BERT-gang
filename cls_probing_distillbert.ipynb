{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38064bitvenvvenv1d7cf1dc08ad48a693d8304396dec1ba",
   "display_name": "Python 3.8.0 64-bit ('.venv': venv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import fine_tune\n",
    "from tqdm import tqdm\n",
    "from transformers import (DistilBertForSequenceClassification,\n",
    "                          DistilBertTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = 'qnli'\n",
    "TEACHER_EXP = 'teacher_base'\n",
    "TMODEL = 'bert'\n",
    "TCKPT = 9822\n",
    "STUDENT_EXP = 'distill_bert_base'\n",
    "SMODEL = 'bert'\n",
    "SCKPT = 9000\n",
    "DATASET = 'train'\n",
    "SDEVICE = 0\n",
    "TDEVICE = 0\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n+---------------------------------------+\n| configuration     | value             |\n+---------------------------------------+\n| accum_step        | 2                 |\n| amp               | 0                 |\n| batch_size        | 32                |\n| beta1             | 0.9               |\n| beta2             | 0.999             |\n| ckpt_step         | 1000              |\n| dataset           | train             |\n| device_id         | 0                 |\n| dropout           | 0.1               |\n| eps               | 1e-08             |\n| experiment        | teacher_base      |\n| log_step          | 500               |\n| lr                | 3e-05             |\n| max_norm          | 1.0               |\n| max_seq_len       | 128               |\n| model             | bert              |\n| num_class         | 2                 |\n| ptrain_ver        | bert-base-uncased |\n| seed              | 42                |\n| task              | qnli              |\n| total_step        | 9822              |\n| warmup_step       | 3274              |\n| weight_decay      | 0.01              |\n+---------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "teacher_config = fine_tune.config.TeacherConfig.load(\n",
    "    experiment=TEACHER_EXP,\n",
    "    model=TMODEL,\n",
    "    task=TASK\n",
    ")\n",
    "teacher_config.device_id = TDEVICE\n",
    "teacher_config.dataset = DATASET\n",
    "print(teacher_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021/05/05 17:01:08 - INFO - fine_tune.task -   Start loading task QNLI dataset train.\n",
      "Loading QNLI train: 104743it [00:00, 462141.77it/s]\n",
      "2021/05/05 17:01:08 - INFO - fine_tune.task -   Number of samples: 104743\n",
      "2021/05/05 17:01:08 - INFO - fine_tune.task -   Finish loading task QNLI dataset train.\n"
     ]
    }
   ],
   "source": [
    "dataset = fine_tune.util.load_dataset_by_config(\n",
    "    config=teacher_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_tknr = fine_tune.util.load_teacher_tokenizer_by_config(\n",
    "    config=teacher_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/kychen/Desktop/BERT-gang/data/fine_tune_experiment/teacher_base_bert_qnli\n"
     ]
    }
   ],
   "source": [
    "TEACHER_EXP_NAME = fine_tune.config.BaseConfig.experiment_name(\n",
    "    experiment=teacher_config.experiment,\n",
    "    model=teacher_config.model,\n",
    "    task=teacher_config.task\n",
    ")\n",
    "TEACHER_EXP_DIR = os.path.join(\n",
    "    fine_tune.path.FINE_TUNE_EXPERIMENT,\n",
    "    TEACHER_EXP_NAME\n",
    ")\n",
    "print(TEACHER_EXP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "teacher_model = fine_tune.util.load_teacher_model_by_config(\n",
    "    config=teacher_config\n",
    ")\n",
    "teacher_model.load_state_dict(\n",
    "    torch.load(\n",
    "        os.path.join(TEACHER_EXP_DIR, f'model-{TCKPT}.pt'),\n",
    "        map_location=teacher_config.device\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_tknr = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/kychen/Desktop/BERT-gang/data/fine_tune_experiment/distill_bert_base_bert_qnli\n"
     ]
    }
   ],
   "source": [
    "STUDENT_EXP_NAME = fine_tune.config.BaseConfig.experiment_name(\n",
    "    experiment=STUDENT_EXP,\n",
    "    model=SMODEL,\n",
    "    task=TASK\n",
    ")\n",
    "STUDENT_EXP_DIR = os.path.join(\n",
    "    fine_tune.path.FINE_TUNE_EXPERIMENT,\n",
    "    STUDENT_EXP_NAME\n",
    ")\n",
    "print(STUDENT_EXP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "student_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    return_dict = True\n",
    ").to(SDEVICE)\n",
    "student_model.load_state_dict(\n",
    "    torch.load(\n",
    "        os.path.join(STUDENT_EXP_DIR, f'model-{SCKPT}.pt'),\n",
    "        map_location=f'cuda:{SDEVICE}'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    collate_fn=dataset.create_collate_fn(),\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = torch.nn.CosineSimilarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_similar_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 104743/104743 [29:55<00:00, 58.33it/s]\n"
     ]
    }
   ],
   "source": [
    "student_model.eval()\n",
    "teacher_model.eval()\n",
    "for text, text_pair, label in tqdm(dataloader):\n",
    "    teacher_encode = teacher_tknr(\n",
    "        text=text,\n",
    "        text_pair=text_pair,\n",
    "        padding='max_length',\n",
    "        max_length=teacher_config.max_seq_len,\n",
    "        return_tensors='pt',\n",
    "        truncation=True\n",
    "    )\n",
    "    teacher_input_ids = teacher_encode['input_ids']\n",
    "    teacher_token_type_ids = teacher_encode['token_type_ids']\n",
    "    teacher_attention_mask = teacher_encode['attention_mask']\n",
    "\n",
    "    student_encode = student_tknr(\n",
    "        text=text,\n",
    "        text_pair=text_pair,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_tensors='pt',\n",
    "        truncation=True\n",
    "    )\n",
    "    student_input_ids = student_encode['input_ids']\n",
    "    student_attention_mask = student_encode['attention_mask']\n",
    "\n",
    "    _, t_cls, _ = teacher_model(\n",
    "        input_ids = teacher_input_ids.to(teacher_config.device),\n",
    "        token_type_ids = teacher_token_type_ids.to(teacher_config.device),\n",
    "        attention_mask = teacher_attention_mask.to(teacher_config.device),\n",
    "        return_hidden_and_attn = True\n",
    "    )\n",
    "    t_cls = t_cls[-1][:,0,:]\n",
    "\n",
    "    output = student_model(\n",
    "        input_ids = student_input_ids.to(f'cuda:{SDEVICE}'),\n",
    "        attention_mask = student_attention_mask.to(f'cuda:{SDEVICE}'),\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "\n",
    "    s_cls = output.hidden_states[-1][:,0,:]\n",
    "    \n",
    "    if cosine_sim(t_cls.to(f'cuda:{SDEVICE}'), s_cls) < 0.5:\n",
    "        not_similar_list.append({'text':text, 'text_pair':text_pair, 'label': label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "104517"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "len(not_similar_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2677.9642810000005"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "len(dataset) * (0.989164-0.963597)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}